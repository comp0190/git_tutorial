{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash, Git, Hyperparameter Tuning and Logging Experiments\n",
    "\n",
    "### By Kimberly Ton-Mai, Muhammad Chaudhry and Sahan Bulathwela\n",
    "\n",
    "This week we are going to learn about a few ideas that will allow you to execute the development part of your thesis projects more successful. We are going to learn to about a few tools, namely, \n",
    "\n",
    "- Bash commands\n",
    "- Git commands\n",
    "- Doing hyperparameter tuning\n",
    "- Logging relevant information when training models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "There are several learning goals behind this week's exercise. You will familiarise with how to:\n",
    "\n",
    "\n",
    "- Use [github](https://github.com/) to manage your code\n",
    "- Use `bash` commands and `git` commands to add new changes to your code while keeping track of them. \n",
    "- Training a machine learning model in your personal computer\n",
    "- Adding *hyperparameter tuning* to your code\n",
    "- Logging the hyperparameters and metrics to track your experimental settings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Plan\n",
    "\n",
    "Today's exercise consists of several steps that need to be executed sequencially to obtain the final code that will conduct hyperparamter tuning for your machine learning model. \n",
    "\n",
    "- Step 1: Forking a public github repository to your own git profile\n",
    "- Step 2: Pulling the git repository to your local environment\n",
    "- Step 3: Do the instructed changes to your code to introduce hyperparameter tuning\n",
    "- Step 4: Add logging to record the hyperparamters and the metrics \n",
    "- Step 5: Commit the changes and push them to a remote git repository. \n",
    "- Step 6: Submit a pull request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Forking a public github repository to your own git profile\n",
    "\n",
    "- Sign into your [github account](https://github.com/) or register with github over [here](https://github.com/signup) \n",
    "- Fork the tutorial code found [here](https://github.com/comp0190/git_tutorial) to create your own version of the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pulling the git repository to your local environment\n",
    "\n",
    "- Firstly, install Git on your local machine. You can find instructions for doing this [here](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\n",
    "- Then go to your desire folder and create a directory named `hyperparameter_tuning_tutorial` using *bash* commands. \n",
    "- Use the relevant git command to `clone` the repository into the directory you just created. \n",
    "- Finally, create a ***new branch*** named `hyperparams_and_logging` from the `master` branch of your repository before doing changes to the code. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Do the instructed changes to your code to introduce hyperparameter tuning\n",
    "\n",
    "- Open this notebook using the Jupyter notebook software. \n",
    "- Instructions on installing Anaconda that contains Jupyter notebook editor is found [here](https://docs.anaconda.com/anaconda/install/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Relevant Python Modules\n",
    "\n",
    "There are several Python modules that we we aim to use today. Let us import them here. We use numpy and pandas for data manipulation. We use scikit learn for splitting data into train and test splits and implement grid search. We use xgboost to implement the desired machine learning model. We use logging library to log information. \n",
    "\n",
    "***Hint:*** You may use the `pip` python package manager to install these libraries in your local python environment. \n",
    "\n",
    "```\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install scikit-learn\n",
    "pip install xgboost\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "We use a publicly available dataset for this exercise which is provided with the data. The dataset we use is provided with the repository and is a Comma-Seperated Values (CSV) file called `concrete_data.csv`. \n",
    "\n",
    "- Let us load the data to our notebook now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset\n",
    "\n",
    "# -- Insert your code here -- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first few lines of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the dataset\n",
    "\n",
    "The objective of this task is to use features of concrete to predict its ***strength***. This makes it a regression problem that falls under the supervised learning domain. \n",
    "\n",
    "- Now let us reshape the data to have the features and the labels of the dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the training data and targets\n",
    "X = data.iloc[:, :8].values\n",
    "Y = data.iloc[:, 8].values.reshape(-1,1) # the strength column is the label. \n",
    "\n",
    "# Preview the size of the feature matrix and the label matrix/ vector\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test split\n",
    "\n",
    "We use the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) in scikit-learn python library to split the data into the train and test splits. \n",
    "\n",
    "- Let us split the data in a way that the *training dataset gets* ***80%*** of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the train and test split \n",
    "\n",
    "# -- Insert your code here -- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the data\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(Y_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Model\n",
    "\n",
    "As mentioned earlier, we are going to use the [XGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn) model to tackle this prediction task. \n",
    "\n",
    "- Let us instatiate the model as `xgb_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(random_state = 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the hyperparameter values we want to explore for the grid search\n",
    "\n",
    "Xgboost model, like many other models have hyperparameters that guide the parameter learning process. In the context of xgboost, there are a few hyperprameters such as,\n",
    "- n_estimators\n",
    "- max_depth\n",
    "- gamma\n",
    "- learning_rate\n",
    "\n",
    "that are important to tune. \n",
    "\n",
    "We create a dictionary where we specify the different hyperparameter values that we want to explore for each hyperparameter of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of the combination of hyperparameter values that you want to apply Grid Search on\n",
    "# Keys are hyperpararmeter names and values are the different set of values for each hyperparameter\n",
    "search_space = {\n",
    "    \"n_estimators\" : [100, 200, 500],\n",
    "    \"max_depth\" : [3, 6, 9],\n",
    "    \"gamma\" : [0.01, 0.1],\n",
    "    \"learning_rate\" : [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Introducing the grid search to the code\n",
    "\n",
    "Now that we have determined the set of hyperparameter values we want to consider, we specify the grid search mechanism we want to use in order to find the best hyperparameter combination that gives best performance for this dataset when using the xgboost model. \n",
    "\n",
    "- Use the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class to explore the performance of our model across different values of the hyperparameters we defined in the search space above.\n",
    "- Name the instantiated grid search object as `GS`.\n",
    "- Use `r2` and `neg_root_mean_squared_error` to report two differnt evaluation metrics that can be used to assess the goodness of fit of a regression model. The different scorers available in scikit-learn are found [here](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- Set the number of cross-validation folds to 5. More info about k-fold cross validation found [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py)\n",
    "\n",
    "***Hint:*** Use the documentation to understand the role of the different parameters and use the appropriate set to define the grid search correctly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the code for cross-validation Grid Search \n",
    "\n",
    "# -- Insert your code here -- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the training data. \n",
    "\n",
    "***Note: This may take a few minutes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "GS.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the results for best parameter values\n",
    "\n",
    "Once the grid search is complete. We can access different attributes populated in the [GridSearchCV](https://scikit-learn.org/stable/modules/g\n",
    "enerated/sklearn.model_selection.GridSearchCV.html) object to explore the results obtained from the grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For example, the model object of the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GS.best_estimator_) # to get the complete details of the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best performing hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GS.best_params_) # to get only the best hyperparameter values that we searched for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The score obtained for the best hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GS.best_score_) # score according to the metric we passed in refit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the results to a CSV for further analysis\n",
    "\n",
    "Once we get the grid search results, we can further export it into a CSV file to further analyse the results using a different tool (e.g. Excel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the grid search results to a csv file for analysis \n",
    "df = pd.DataFrame(GS.cv_results_)\n",
    "df = df.sort_values(\"rank_test_r2\")\n",
    "df.to_csv(\"cv_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hint***: From the analysis of the model's accuracy across different hyperparameter values, you may have noticed that the best model with 500 trees reaches an r-squared value of 0.9228 while the model with 200 trees reaches 0.9221. You have to make a choice if the computational power of 300 more trees in the model is worth this tiny improvement in the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add logging to record the hyperparamters and the metrics \n",
    "\n",
    "- Instead of printing the best set of hyperparameter values, use logging to add log messages that can capture these values. \n",
    "\n",
    "***Hint:*** We can use the `INFO` logging level to report such values. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the logger\n",
    "log = logging.getLogger(\"my-logger\")\n",
    "\n",
    "# Submit log entry\n",
    "log.info(\"Best Gamma Value is: {}\".format(GS.best_params_[\"gamma\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Report the remaining hyperprameter values (`learning_rate`, `max_depth` and `n_estimators`) using the `info` log level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add logging statements\n",
    "\n",
    "# -- Insert your code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Commit the changes and push them to a remote git repository. \n",
    "\n",
    "- Use the relevant git command to *commit* the code into the local version of your repository\n",
    "- Use the relevant git command to *push* the committed code into the `hyperparams_and_logging` branch of the remote repository (in github)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Submit a pull request\n",
    "\n",
    "- Use the github web user interface to submit a ***pull request*** to your repository. Details found [here](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request)  \n",
    "\n",
    "***Hint:*** Details about pushing the new code to the branch is also found [here](https://comp0190.github.io/lectures/topics/3_tuning/tutorial.html#committing-your-changes-and-pushing-to-your-remote-repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)  Week 1 Cont... \n",
    "### Bayesian Search Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore how Scikit Bayesian Optimisation works with [Skopt](https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html). In dthe documentation you can see how it works with different Acquisition functions for choosing the next point to query the original function.\n",
    "\n",
    "***Hint:*** You need to install a new python library called `skopt`\n",
    "\n",
    "```\n",
    "pip install scikit-optimize\n",
    "```\n",
    "\n",
    "- We will be defining the search space and using skopt.BayesSearchCV class to optimise the hyperparamters. You can see how this class samples and explores hyperparameter values from specific distributions [here](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant modules for Bayesian Optimisation \n",
    "\n",
    "# -- Insert your code here --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the search space for Bayesian Optimisation \n",
    "search_space_bo = {\n",
    "    \"n_estimators\" : Integer(100,500),\n",
    "    \"max_depth\" : Integer(4, 400), \n",
    "    \"gamma\" : (1e-2, 0.1, \"log-uniform\"),\n",
    "    \"learning_rate\" : (0.0001, 0.1, \"log-uniform\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Bayes search object\n",
    "\n",
    "# -- Insert your code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model based on Bayes Search CV object we defined earlier. Note: this may take a few minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "BO.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the results of best hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BO.best_score_) # score according to the metric we passed in refit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BO.best_params_) # to get only the best hyperparameter values that we searched for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BO.best_estimator_) # to get the complete details of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the Bayesian Optimization results to a csv file for analysis \n",
    "df1 = pd.DataFrame(BO.cv_results_)\n",
    "df1 = df1.sort_values(\"mean_test_score\")\n",
    "df1.to_csv(\"bo_cv_results.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0190",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "633d94998119a4c188dfbd34cb0612f2cda18710129c72b5fc0a03a00b1196cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
